\section{Methods}
\label{sec:methods}
%
\subsection{Methodological background}
\label{sec:methods_background}

We suggest clustering the current methodologies of template-based segmentation 
methods into three groups. The first group typically adds a shape prior term to 
the energy functional of an evolving active contour
\citep{bresson_variational_2006,chan_level_2005,chen_using_2002,cremers_kernel_2006,
gastaud_combining_2004,paragios_level_2003,vemuri_joint_2003,yezzi_variational_2003}.
These methods generally have a explicit description of the expected relative boundary 
locations of the object to be delineated, and some even model the statistical deviations
from this average shape. Closely related to this group are atlas-based segmentation
methods \citep{gorthi_segmentation_2009,gorthi_active_2011,pohl_unifying_2005,
pohl_bayesian_2006,wang_joint_2006}, where the prior 
imposes consistent voxel-based classification of contiguous regions. Here, the 
presence of more structures than one unique \gls{roi} helps aligning the target image 
with the atlas in a hierarchical fashion. Finally, the third group generalizes 
the atlas to actual images, and the contour is to segment simultaneously two 
different target images, related by a spatial transform to be co-estimated
\citep{wyatt_map_2003,yezzi_variational_2003}.

\subsection{Mumford-Shah Functional derivation from the Maximum A-Posteriori Model}
\label{sec:methods_map}
%
A widely used approach to image segmentation is derived from the
Bayes' rule \eqref{eq:bayes_rule}, where one seeks for a partitioning
of a certain image $X$ in piecewise smooth regions $\Omega = \lbrace \Omega_k , 
k\in\left[ 1 .. K \right] \rbrace$,  that maximizes the a posteriori 
probability given the multivariate image $X \in \mathbb{R}^C$, 
with $C$ being the number of image channels.
\begin{equation}
p(Y \mid X) = \frac{p(X \mid Y)\, p(Y)}{p(X)}.
\label{eq:bayes_rule}
\end{equation}

Therefore, $Y$ is a certain realization of the piecewise 
disjoint region set $\Omega$. $p(X \mid Y)$ is the \emph{likelihood} of 
the realization of $X$ (the image) given a certain distribution model for 
each region $\Omega_k$. The second term, $p(Y)$, is the a-priori probability of 
the partitioning $Y$. Finally, $p(X)$ is the probability of a certain image 
realization, and thus, it will remain constant when computing the \gls{map}.
Consequently, $p(Y \mid X) \propto p(X \mid Y)\, p(Y)$, and:
\begin{equation}
\underset{Y}{\argmax} \left\{ p(Y \mid X) \right\} = 
\underset{Y}{\argmax} \left\{ p(X \mid Y)\, p(Y) \right\}.
\end{equation}


An extended assumption is that the feature vector realization $X$ is
\emph{i.i.d.}, and thus, it is possible to write the a-posteriori
probability $p(X \mid Y)$ as a continuous product with $d\mathbf{x}$ the
infinitesimal voxel size:
\begin{equation}
p(X \mid Y) \, p(Y) = \underset{k}{\prod} \underset{\mathbf{x}\in \Omega_k}{\prod}
p_k( X(\mathbf{x}) \mid Y(\mathbf{x}) )^{d\mathbf{x}},
\label{eq:bayes_aposteriori}
\end{equation}
where the prior probability $p(Y)$ is implicitly
defined by the regions definition. 


A second widely-accepted assumption is the multivariate normal 
distribution of the different tissues in \gls{mri} data. Therefore,
the posterior probability of an infinitesimal voxel
can be written as:
\begin{equation}
p_k( X(\mathbf{x}) \mid Y(\mathbf{x}) ) = \frac{1}{ \sqrt{(2\pi)^{C}\,\left|\boldsymbol{\Sigma}_{k}\right|}}\,{e^{\left(-\frac{1}{2}  \Delta^2_k (\mathbf{X(\mathbf{x})}) \right)}}.
\label{eq:bayes_mpdf}
\end{equation}
where we can identify the factor in the exponential, with 
$\mathbf{f} = X(\mathbf{x})$ as the squared \emph{Mahalanobis 
distance} with the parameters set $\Theta_k = \lbrace \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \rbrace$:
\begin{equation}
\Delta^2_k (\mathbf{f} \mid \Theta_k ) = (\mathbf{f} - \boldsymbol{\mu}_k)^T \, \boldsymbol{\Sigma}^{-1}_k \, (\mathbf{f} - \boldsymbol{\mu}_k).
\label{eq:bayes_mahalanobis}
\end{equation}

Finally, we can turn the \gls{map} problem into a variational one
applying the following log-transform:
\begin{multline}
E(X \mid Y)= -\log \left[ p(X \mid Y) \, p(Y) \right] = \\
= -\log \left[ \underset{k}{\prod} \underset{\mathbf{x}\in \Omega_k}{\prod}
p_k( X(\mathbf{x}) \mid Y(\mathbf{x}) )^{d\mathbf{x}} \right] = \\
= \sum\limits_k \int_{\Omega_k} -\log \left[ p_k(X(\mathbf{x}) \mid Y(\mathbf{x} ) ) \right] \, d\mathbf{x},
\label{eq:energy_1}
\end{multline}
and introducing the posterior probability term \eqref{eq:bayes_mpdf}, 
we can express the functional in terms of $\lbrace\Theta_k,\Omega_k\rbrace$:
\begin{multline}
E(\Theta_k,\Omega_k) = \\
= \sum\limits_k \int_{\Omega_k} -\log \left[ \frac{1}{ \sqrt{(2\pi)^{C}\,\left|\boldsymbol{\Sigma}_{k}\right|}}\,{e^{\left(-\frac{1}{2}  \Delta^2_k (\mathbf{f}) \right)}} \right] \, d\mathbf{x} = \\
= \sum\limits_k \int_{\Omega_k} \left[ \frac{1}{2} \log{ \left( (2\pi)^{C}\,\left|\boldsymbol{\Sigma}_{k}\right| \right)} + \frac{1}{2}  \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x}.
\end{multline}
Finally, after removing scaling factors and independent constants,
we obtain:
\begin{align}
E(\Theta_k,\Omega_k) = \sum\limits_k \int_{\Omega_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x},
\label{eq:map_energy}
\end{align}
(\textbf{FIXME:} bad explanation) where we have a constant term scaled by
the total volume of the partition $\Omega_k$ and the determinant of the 
covariance matrix of the partition $\left|\boldsymbol{\Sigma}_{k}\right|$,
plus an energy term based on the squared \emph{Mahalanobis distance}.

Equation \eqref{eq:map_energy} resembles the Mumford-Shah functional including variance,
that modifies the original functional in a way that it can deal with more general
distributions. This is necessary to avoid the assumption that regions $\Omega_k$
have a fixed covariance matrix on their complete domain. One immediate advantage
of this functional from the original one is the possibility to distinguish regions
with the same mean vector but different covariance matrix \citep{brox_local_2009}:
\begin{multline}
E(\Theta_k,\Omega_k) = \sum\limits_k \int_{\Omega_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x} \\
+ \lambda \int_{\Omega_k - K}  ( \left| \nabla \mathbf{\mu} \right| ^2 + \left| \nabla \mathbf{\Sigma}_k \right| ^2 ) \, d\mathbf{x} 
+ \nu |K|,
\end{multline}
that is easily identifiable with \eqref{eq:map_energy} when we apply 
the so-called \emph{cartoon limit}, 
for $\lambda \to \infty$:
\begin{equation}
E(\Theta_k,K) = \sum\limits_k \int_{\Omega_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x}
+ \nu |K|.
\end{equation}

As long as we do not penalize the edge set $K$ length, $\nu = 0$ and
the result is dual to \eqref{eq:map_energy}:
\begin{equation}
E(\Theta_k,K) = \sum\limits_k \int_{\Omega_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x}.
\end{equation}


\subsection{Deformation model}
\label{sec:deformation}
%
The segmentation problem is transformed into a registration one if the initial
partition $Y$ is derived from the initial shape-priors in reference space. 
Introducing a dense deformation field $u$ that maps the original partition to
a better fit of the regions in the target coordinate space. Thus, the minimization
problem stated in \eqref{eq:map_energy} can be expressed so that $u$ is the 
unknown:
\begin{equation}
E(u(\mathbf{x})) = \sum\limits_k \int_{\Omega'_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f'}) \right] \,d\mathbf{x}
\label{eq:map_energy_deformation}
\end{equation}
where $\Omega'_k = u(\Omega_k)$ and $\mathbf{f'} = X(u(\mathbf{x}))$.



\subsection{\Acrlong{acwe} based segmentation model}
%
Let us denote $\{c_i\}_{i=1..N_c}$ the nodes of a shape prior surface. In
our application, a precise \gls{wm}-\gls{gm} interface extracted from a
high-resolution reference volume. All the formulations can be naturally
extended to include more shape priors. On the other hand, we have a 
number of \gls{dwi}-derived features at each
voxel of the volume. Let us denote by $x$ the voxel and 
$f(x) = [ f_1, f_2, \ldots, f_N]^T(x)$ its associated feature vector.\\
%
The transformation from reference into \gls{dwi} coordinate space is 
achieved through a dense deformation field $u(x)$, such that:
%
\begin{equation}
c_i' = T\{c_i\} = c_i + u(c_i)
\end{equation}
% 
Since the nodes of the anatomical surfaces might lay off-grid, it is 
required to derive $u(x)$ from a discrete set of parameters $\{u_k\}_{k=1..K}$.
Densification is achieved through a set of associated basis functions 
$\Psi_k$ (e.g. rbf, interpolation splines):
%
\begin{equation}
u(x) = \sum_k \Psi_k(x) u_k
\end{equation}
%
Consequently, the transformation writes
%
\begin{equation}
\label{eq:transformation}
c_i' = T\{c_i\} = c_i + u(c_i) = c_i + \sum_k \Psi_k(c_i)u_k
\end{equation} 
%
% Comment: maybe this is not for IPMI 2013.
%Note that, since $c_i$ remains constant in the DW segmentation process,
%the values of $\Psi_k(c_i)$ can be precomputed. Also, provided compact 
%support of the basis functions, the system remains relatively sparse.\\
%
Based on the current estimate of the distortion $u$, we can compute 
``expected samples'' within the shape prior projected into the \gls{dwi}.
Thus, we now estimate region descriptors of the \gls{dwi} features 
$f(x)$ of the regions defined by the priors in \gls{dwi} space.
%
Using Gaussian distributions as region descriptors, we propose an
\gls{acwe}-like, piece-wise constant, variational image segmentation
model (where the unknown is the deformation field)
\cite{chan_active_2001}:

where $R$ indexes the existing regions and the integral domains
depend on the deformation field $u$. Note
that minimizing this energy, $\argmin_u\{E\}$, yields the \gls{map} 
estimate of a piece-wise smooth image model affected by Gaussian 
additive noise. This inverse problem is ill-posed
\cite{bertero_ill-posed_1988,hadamard_sur_1902}.
In order to account for deformation field regularity and to render the 
problem well-posed, we include limiting and regularization terms into 
the energy functional \cite{morozov_linear_1975,tichonov_solution_1963}:
%
\begin{align}
\label{eq:complete_energy}
E(u) &= \sum_{\forall{R}} \lbrace \int_{\Omega_R} (f-\mu_R)^T\Sigma_R^{-1}(f-\mu_R) dx \rbrace \nonumber \\
&\quad + \alpha \int  \|u\|^2 dx + \beta \int \left( \|\nabla u_x\|^2 + \|\nabla u_y\|^2 + \|\nabla u_z\|^2\right) dx
\end{align}
%
These regularity terms ensure that the segmenting contours in 
\gls{dwi} space are still close to their native shape. The model
easily allows to incorporate inhomogeneous and anisotropic 
regularization \cite{nagel_investigation_1986} to better regularize
the \gls{epi} distortion. \\
%

At each iteration, we update the distortion along the steepest 
energy descent. This gradient descent step can be efficiently 
tackled by discretizing the time in a forward Euler scheme, 
and making the right hand side semi-implicit in the 
regularization terms:
%
\begin{align}
\frac{u^{t+1}-u^t}{\tau} &= - \sum_{i=1}^{N_c} \left[ e(f(c_i'))  \hat{n}_{c_i'} \Psi_{c_i}(x) \right] -\alpha u^{t+1} + \beta\Delta u^{t+1}
\end{align}
%
where the data terms remain functions of the current estimate 
$u^t$, thus $c_i' = c_i'(u^t)$. For simplicity on notation, we 
restricted the number of priors to only 1. We also defined 
$e(f(c_i')) = E_{out}(f(c_i')) - E_{in}(f(c_i'))$, 
and $E_R(f) = {(f-\mu_R)^T\Sigma_R^{-1}(f-\mu_R)}$.
We applied a spectral approach to solve this implicit scheme:
%
\begin{equation}
u^{t+1} = \mathcal{F}^{-1}\left\{ \frac{\mathcal{F}\{u^t/\tau
- \sum_{i=1}^{N_c} \left[e(f(c_i')) \hat{n}_{c_i'} \Psi_{c_i}(x) \right]  \}}{\mathcal{F}\{(1/\tau+\alpha)\mathcal{I}-\beta\Delta\}} \right\}
\end{equation}
%

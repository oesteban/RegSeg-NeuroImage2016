\section{Methods}
\label{sec:methods}
%
\subsection{Maximum A-Posteriori Model}
\label{sec:methods_map}
%
A widely used approach to image segmentation is derived from the
Bayes' rule (\autoref{eq:bayes_rule}), where one seeks for a partitioning
of a certain image $X$ in piecewise smooth regions $\Omega = \lbrace \Omega_k , 
k\in\left[ 1 .. K \right] \rbrace$,  that maximizes the a posteriori 
probability given the multivariate image $X \in \mathbb{R}^C$, 
with $C$ being the number of image channels.
\begin{equation}
p(Y \mid X) = \frac{p(X \mid Y)\, p(Y)}{p(X)}.
\label{eq:bayes_rule}
\end{equation}

Therefore, $Y$ is a certain realization of the piecewise 
disjoint region set $\Omega$. $p(X \mid Y)$ is the \emph{likelihood} of 
the realization of $X$ (the image) given a certain distribution model for 
each region $\Omega_k$. The second term, $p(Y)$, is the a-priori probability of 
the partitioning $Y$. Finally, $p(X)$ is the probability of a certain image 
realization, and thus, it will remain constant when computing the \gls{map}.
Consequently, $p(Y \mid X) \propto p(X \mid Y)\, p(Y)$, and:
\begin{equation}
\underset{Y}{\argmax} \left\{ p(Y \mid X) \right\} = 
\underset{Y}{\argmax} \left\{ p(X \mid Y)\, p(Y) \right\}.
\end{equation}


An extended assumption is that the feature vector realization $X$ is
\emph{i.i.d.}, and thus, it is possible to write the a-posteriori
probability $p(X \mid Y)$ as a continuous product with $d\mathbf{x}$ the
infinitesimal voxel size:
\begin{equation}
p(X \mid Y) \, p(Y) = \underset{k}{\prod} \underset{\mathbf{x}\in \Omega_k}{\prod}
p_k( X(\mathbf{x}) \mid Y(\mathbf{x}) )^{d\mathbf{x}},
\label{eq:bayes_aposteriori}
\end{equation}
where the prior probability $p(Y)$ is implicitly
defined by the regions definition. 


A second widely-accepted assumption is the multivariate normal 
distribution of the different tissues in \gls{mri} data. Therefore,
the posterior probability of an infinitesimal voxel
can be written as:
\begin{equation}
p_k( X(\mathbf{x}) \mid Y(\mathbf{x}) ) = \frac{1}{ \sqrt{(2\pi)^{C}\,\left|\boldsymbol{\Sigma}_{k}\right|}}\,{e^{\left(-\frac{1}{2}  \Delta^2_k (\mathbf{X(\mathbf{x})}) \right)}}.
\label{eq:bayes_mpdf}
\end{equation}
where we can identify the factor in the exponential, with 
$\mathbf{f} = X(\mathbf{x})$ as the squared \emph{Mahalanobis 
distance}:
\begin{equation}
\Delta^2_k (\mathbf{f}) = (\mathbf{f} - \boldsymbol{\mu}_k)^T \, \Sigma^{-1}_k \, (\mathbf{f} - \boldsymbol{\mu}_k).
\label{eq:bayes_mahalanobis}
\end{equation}

Finally, we can turn the \gls{map} problem into a variational one
applying the following log-transform:
\begin{align}
E(X \mid Y)= -\log \left[ p(X \mid Y) \, p(Y) \right] = \\
= -\log \left[ \underset{k}{\prod} \underset{\mathbf{x}\in \Omega_k}{\prod}
p_k( X(\mathbf{x}) \mid Y(\mathbf{x}) )^{d\mathbf{x}} \right] = \\
= \sum\limits_k \int_{\Omega_k} -\log \left[ p_k(X(\mathbf{x}) \mid Y(\mathbf{x} ) ) \right] \, d\mathbf{x},
\label{eq:energy_1}
\end{align}
and introducing the posterior probability term (\autoref{eq:bayes_mpdf}), we get:
\begin{align}
E(X \mid Y) = \sum\limits_k \int_{\Omega_k} -\log \left[ \frac{1}{ \sqrt{(2\pi)^{C}\,\left|\boldsymbol{\Sigma}_{k}\right|}}\,{e^{\left(-\frac{1}{2}  \Delta^2_k (\mathbf{f}) \right)}} \right] \, d\mathbf{x} = \\
= \sum\limits_k \int_{\Omega_k} \left[ \frac{1}{2} \log{ \left( (2\pi)^{C}\,\left|\boldsymbol{\Sigma}_{k}\right| \right)} + \frac{1}{2}  \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x}.
\end{align}
Finally, after removing scaling factors and independent constants,
we obtain
\begin{align}
E(X \mid Y) = \sum\limits_k \left[ \log{\left|\boldsymbol{\Sigma}_{k}\right|} \int_{\Omega_k} d\mathbf{x} + \int_{\Omega_k} \Delta^2_k (\mathbf{f}) \,d\mathbf{x} \right],
\label{eq:map_energy}
\end{align}
(\textbf{FIXME:} bad explanation) where we have a constant term scaled by
the total volume of the partition $\Omega_k$ and the determinant of the 
covariance matrix of the partition $\left|\boldsymbol{\Sigma}_{k}\right|$,
plus an energy term based on the squared \emph{Mahalanobis distance}.



\subsection{Deformation model}
\label{sec:deformation}
%
The segmentation problem is transformed into a registration one if the initial
partition $Y$ is derived from the initial shape-priors in reference space. 
Introducing a dense deformation field $u$ that maps the original partition to
a better fit of the regions in the target coordinate space. Thus, the minimization
problem stated in \autoref{eq:map_energy} can be expressed so that $u$ is the 
unknown:
\begin{equation}
E(u(\mathbf{x})) = \sum\limits_k \int_{\Omega'_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f'}) \right] \,d\mathbf{x}
\label{eq:map_energy_deformation}
\end{equation}
where $\Omega'_k = u(\Omega_k)$ and $\mathbf{f'} = X(u(\mathbf{x}))$.



\subsection{\Acrlong{acwe} based segmentation model}
%
Let us denote $\{c_i\}_{i=1..N_c}$ the nodes of a shape prior surface. In
our application, a precise \gls{wm}-\gls{gm} interface extracted from a
high-resolution reference volume. All the formulations can be naturally
extended to include more shape priors. On the other hand, we have a 
number of \gls{dwi}-derived features at each
voxel of the volume. Let us denote by $x$ the voxel and 
$f(x) = [ f_1, f_2, \ldots, f_N]^T(x)$ its associated feature vector.\\
%
The transformation from reference into \gls{dwi} coordinate space is 
achieved through a dense deformation field $u(x)$, such that:
%
\begin{equation}
c_i' = T\{c_i\} = c_i + u(c_i)
\end{equation}
% 
Since the nodes of the anatomical surfaces might lay off-grid, it is 
required to derive $u(x)$ from a discrete set of parameters $\{u_k\}_{k=1..K}$.
Densification is achieved through a set of associated basis functions 
$\Psi_k$ (e.g. rbf, interpolation splines):
%
\begin{equation}
u(x) = \sum_k \Psi_k(x) u_k
\end{equation}
%
Consequently, the transformation writes
%
\begin{equation}
\label{eq:transformation}
c_i' = T\{c_i\} = c_i + u(c_i) = c_i + \sum_k \Psi_k(c_i)u_k
\end{equation} 
%
% Comment: maybe this is not for IPMI 2013.
%Note that, since $c_i$ remains constant in the DW segmentation process,
%the values of $\Psi_k(c_i)$ can be precomputed. Also, provided compact 
%support of the basis functions, the system remains relatively sparse.\\
%
Based on the current estimate of the distortion $u$, we can compute 
``expected samples'' within the shape prior projected into the \gls{dwi}.
Thus, we now estimate region descriptors of the \gls{dwi} features 
$f(x)$ of the regions defined by the priors in \gls{dwi} space.
%
Using Gaussian distributions as region descriptors, we propose an
\gls{acwe}-like, piece-wise constant, variational image segmentation
model (where the unknown is the deformation field)
\cite{chan_active_2001}:

where $R$ indexes the existing regions and the integral domains
depend on the deformation field $u$. Note
that minimizing this energy, $\argmin_u\{E\}$, yields the \gls{map} 
estimate of a piece-wise smooth image model affected by Gaussian 
additive noise. This inverse problem is ill-posed
\cite{bertero_ill-posed_1988,hadamard_sur_1902}.
In order to account for deformation field regularity and to render the 
problem well-posed, we include limiting and regularization terms into 
the energy functional \cite{morozov_linear_1975,tichonov_solution_1963}:
%
\begin{align}
\label{eq:complete_energy}
E(u) &= \sum_{\forall{R}} \lbrace \int_{\Omega_R} (f-\mu_R)^T\Sigma_R^{-1}(f-\mu_R) dx \rbrace \nonumber \\
&\quad + \alpha \int  \|u\|^2 dx + \beta \int \left( \|\nabla u_x\|^2 + \|\nabla u_y\|^2 + \|\nabla u_z\|^2\right) dx
\end{align}
%
These regularity terms ensure that the segmenting contours in 
\gls{dwi} space are still close to their native shape. The model
easily allows to incorporate inhomogeneous and anisotropic 
regularization \cite{nagel_investigation_1986} to better regularize
the \gls{epi} distortion. \\
%

At each iteration, we update the distortion along the steepest 
energy descent. This gradient descent step can be efficiently 
tackled by discretizing the time in a forward Euler scheme, 
and making the right hand side semi-implicit in the 
regularization terms:
%
\begin{align}
\frac{u^{t+1}-u^t}{\tau} &= - \sum_{i=1}^{N_c} \left[ e(f(c_i'))  \hat{n}_{c_i'} \Psi_{c_i}(x) \right] -\alpha u^{t+1} + \beta\Delta u^{t+1}
\end{align}
%
where the data terms remain functions of the current estimate 
$u^t$, thus $c_i' = c_i'(u^t)$. For simplicity on notation, we 
restricted the number of priors to only 1. We also defined 
$e(f(c_i')) = E_{out}(f(c_i')) - E_{in}(f(c_i'))$, 
and $E_R(f) = {(f-\mu_R)^T\Sigma_R^{-1}(f-\mu_R)}$.
We applied a spectral approach to solve this implicit scheme:
%
\begin{equation}
u^{t+1} = \mathcal{F}^{-1}\left\{ \frac{\mathcal{F}\{u^t/\tau
- \sum_{i=1}^{N_c} \left[e(f(c_i')) \hat{n}_{c_i'} \Psi_{c_i}(x) \right]  \}}{\mathcal{F}\{(1/\tau+\alpha)\mathcal{I}-\beta\Delta\}} \right\}
\end{equation}
%

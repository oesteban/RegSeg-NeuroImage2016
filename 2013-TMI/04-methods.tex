\section{Methods}
\label{sec:methods}
%
\subsection{Related work}
\label{sec:methods_background}

We suggest clustering the current methodologies of template-based segmentation 
methods into three groups. The first group typically adds a shape prior term to 
the energy functional of an evolving active contour \citep{bresson_variational_2006,c
han_level_2005,chen_using_2002,cremers_kernel_2006,gastaud_combining_2004}.
These methods generally have a explicit description of the expected relative boundary 
locations of the object to be delineated, and some even model the statistical deviations
from this average shape. By including a coordinates mapping, it is possible to perform
active contours based registration between timesteps in a time-series or between different
images \citep{wyatt_map_2003,paragios_level_2003,vemuri_joint_2003,yezzi_variational_2003}.
This second group is closely related to atlas-based segmentation methods 
\citep{gorthi_segmentation_2009,gorthi_active_2011,pohl_unifying_2005,
pohl_bayesian_2006,wang_joint_2006}, where the prior 
imposes consistent voxel-based classification of contiguous regions. Here, the 
presence of more structures than one unique \gls{roi} helps aligning the target image 
with the atlas in a hierarchical fashion. Finally, the third group generalizes 
the atlas to actual images, and the contour is to segment simultaneously two 
different target images, related by a spatial transform to be co-estimated

\subsection{From the Maximum A-Posteriori Model to the Mumford-Shah functional}
\label{sec:methods_map}
%
A widely used approach to image segmentation is derived from the
Bayes' rule \eqref{eq:bayes_rule}, where one seeks for a partitioning
of a certain image $X$ in piecewise smooth regions $\Omega = \lbrace \Omega_k , 
k\in\left[ 1 .. K \right] \rbrace$,  that maximizes the a posteriori 
probability given the multivariate image $X \in \mathbb{R}^C$, 
with $C$ being the number of image channels.
\begin{equation}
p(Y \mid X) = \frac{p(X \mid Y)\, p(Y)}{p(X)}.
\label{eq:bayes_rule}
\end{equation}

Therefore, $Y$ is a certain realization of the piecewise 
disjoint region set $\Omega$. $p(X \mid Y)$ is the \emph{likelihood} of 
the realization of $X$ (the image) given a certain distribution model for 
each region $\Omega_k$. The second term, $p(Y)$, is the a-priori probability of 
the partitioning $Y$. Finally, $p(X)$ is the probability of a certain image 
realization, and thus, it will remain constant when computing the \gls{map}.
Consequently, $p(Y \mid X) \propto p(X \mid Y)\, p(Y)$, and:
\begin{equation}
\underset{Y}{\argmax} \left\{ p(Y \mid X) \right\} = 
\underset{Y}{\argmax} \left\{ p(X \mid Y)\, p(Y) \right\}.
\end{equation}


An extended assumption is that the feature vector realization $X$ is
\emph{i.i.d.}, and thus, it is possible to write the a-posteriori
probability $p(X \mid Y)$ as a continuous product with $d\mathbf{x}$ the
infinitesimal voxel size:
\begin{equation}
p(X \mid Y) \, p(Y) = \underset{k}{\prod} \underset{\mathbf{x}\in \Omega_k}{\prod}
p_k( X(\mathbf{x}) \mid Y(\mathbf{x}) )^{d\mathbf{x}},
\label{eq:bayes_aposteriori}
\end{equation}
where the prior probability $p(Y)$ is implicitly
defined by the regions definition. 


A second widely-accepted assumption is the multivariate normal 
distribution of the different tissues in \gls{mri} data. Therefore,
the posterior probability of an infinitesimal voxel
can be written as:
\begin{equation}
p_k( X(\mathbf{x}) \mid Y(\mathbf{x}) ) = \frac{1}{ \sqrt{(2\pi)^{C}\,\left|\boldsymbol{\Sigma}_{k}\right|}}\,{e^{\left(-\frac{1}{2}  \Delta^2_k (\mathbf{X(\mathbf{x})}) \right)}}.
\label{eq:bayes_mpdf}
\end{equation}
where we can identify the factor in the exponential, with 
$\mathbf{f} = X(\mathbf{x})$ as the squared \emph{Mahalanobis 
distance} with the parameters set $\Theta_k = \lbrace \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \rbrace$:
\begin{equation}
\Delta^2_k (\mathbf{f} \mid \Theta_k ) = (\mathbf{f} - \boldsymbol{\mu}_k)^T \, \boldsymbol{\Sigma}^{-1}_k \, (\mathbf{f} - \boldsymbol{\mu}_k).
\label{eq:bayes_mahalanobis}
\end{equation}

Finally, we can turn the \gls{map} problem into a variational one
applying the following log-transform:
\begin{multline}
E(X \mid Y)= -\log \left[ p(X \mid Y) \, p(Y) \right] = \\
= -\log \left[ \underset{k}{\prod} \underset{\mathbf{x}\in \Omega_k}{\prod}
p_k( X(\mathbf{x}) \mid Y(\mathbf{x}) )^{d\mathbf{x}} \right] = \\
= \sum\limits_k \int_{\Omega_k} -\log \left[ p_k(X(\mathbf{x}) \mid Y(\mathbf{x} ) ) \right] \, d\mathbf{x},
\label{eq:energy_1}
\end{multline}
and introducing the posterior probability term \eqref{eq:bayes_mpdf}, 
we can express the functional in terms of $\lbrace\Theta_k,\Omega_k\rbrace$:
\begin{multline}
E(\Theta_k,\Omega_k) = \\
= \sum\limits_k \int_{\Omega_k} -\log \left[ \frac{1}{ \sqrt{(2\pi)^{C}\,\left|\boldsymbol{\Sigma}_{k}\right|}}\,{e^{\left(-\frac{1}{2}  \Delta^2_k (\mathbf{f}) \right)}} \right] \, d\mathbf{x} = \\
= \sum\limits_k \int_{\Omega_k} \left[ \frac{1}{2} \log{ \left( (2\pi)^{C}\,\left|\boldsymbol{\Sigma}_{k}\right| \right)} + \frac{1}{2}  \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x}.
\end{multline}
Finally, after removing scaling factors and independent constants,
we obtain:
\begin{align}
E(\Theta_k,\Omega_k) = \sum\limits_k \int_{\Omega_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x},
\label{eq:map_energy}
\end{align}
\todo[inline]{improve this explanation} where we have a constant term scaled by
the total volume of the partition $\Omega_k$ and the determinant of the 
covariance matrix of the partition $\left|\boldsymbol{\Sigma}_{k}\right|$,
plus an energy term based on the squared \emph{Mahalanobis distance}.

Equation \eqref{eq:map_energy} resembles the Mumford-Shah functional including variance,
that modifies the original functional in a way that it can deal with more general
distributions. This is necessary to avoid the assumption that regions $\Omega_k$
have a fixed covariance matrix on their complete domain. One immediate advantage
of this functional from the original one is the possibility to distinguish regions
with the same mean vector but different covariance matrix \citep{brox_local_2009}:
\begin{multline}
E(\Theta_k,\Omega_k) = \sum\limits_k \int_{\Omega_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x} \\
+ \lambda \int_{\Omega_k - K}  ( \left| \nabla \mathbf{\mu} \right| ^2 + \left| \nabla \mathbf{\Sigma}_k \right| ^2 ) \, d\mathbf{x} 
+ \nu |K|,
\end{multline}
that is easily identifiable with \eqref{eq:map_energy} when we apply 
the so-called \emph{cartoon limit}, 
for $\lambda \to \infty$:
\begin{equation}
E(\Theta_k,K) = \sum\limits_k \int_{\Omega_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x}
+ \nu |K|.
\end{equation}

As long as we do not penalize the edge set $K$ length, $\nu = 0$ and
the result is dual to \eqref{eq:map_energy}:
\begin{equation}
E(\Theta_k,K) = \sum\limits_k \int_{\Omega_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f}) \right] \,d\mathbf{x}.
\end{equation}


\subsection{Deformation model}
\label{sec:deformation}
%
The segmentation problem is transformed into a registration one if the initial
partition $Y$ is derived from the initial shape-priors in reference space. 
Introducing a dense deformation field $u$ that maps the original partition to
a better fit of the regions in the target coordinate space. Thus, the minimization
problem stated in \eqref{eq:map_energy} can be expressed so that $u$ is the 
unknown:
\begin{equation}
E(u(\mathbf{x})) = \sum\limits_k \int_{\Omega'_k} \left[ \log \left|\mathbf{\Sigma}_k\right| + \Delta^2_k (\mathbf{f'}) \right] \,d\mathbf{x}
\label{eq:map_energy_deformation}
\end{equation}
where $\Omega'_k = u(\Omega_k)$ and $\mathbf{f'} = X(u(\mathbf{x}))$.



\subsection{\Acrlong{acwe} based segmentation model}
%
Let us denote $\{c_i\}_{i=1..N_c}$ the nodes of a shape prior surface. In
our application, a precise \gls{wm}-\gls{gm} interface extracted from a
high-resolution reference volume. All the formulations can be naturally
extended to include more shape priors. On the other hand, we have a 
number of \gls{dwi}-derived features at each
voxel of the volume. Let us denote by $x$ the voxel and 
$f(x) = [ f_1, f_2, \ldots, f_N]^T(x)$ its associated feature vector.\\
%
The transformation from reference into \gls{dwi} coordinate space is 
achieved through a dense deformation field $u(x)$, such that:
%
\begin{equation}
c_i' = T\{c_i\} = c_i + u(c_i)
\end{equation}
% 
Since the nodes of the anatomical surfaces might lay off-grid, it is 
required to derive $u(x)$ from a discrete set of parameters $\{u_k\}_{k=1..K}$.
Densification is achieved through a set of associated basis functions 
$\psi_k$ (e.g. rbf, interpolation splines):
%
\begin{equation}
u(x) = \sum_k \psi_k(x) u_k
\end{equation}
%
Consequently, the transformation writes
%
\begin{equation}
\label{eq:transformation}
c_i' = T\{c_i\} = c_i + u(c_i) = c_i + \sum_k \psi_k(c_i)u_k
\end{equation} 
%
% Comment: maybe this is not for IPMI 2013.
%Note that, since $c_i$ remains constant in the DW segmentation process,
%the values of $\psi_k(c_i)$ can be precomputed. Also, provided compact 
%support of the basis functions, the system remains relatively sparse.\\
%
Based on the current estimate of the distortion $u$, we can compute 
``expected samples'' within the shape prior projected into the \gls{dwi}.
Thus, we now estimate region descriptors of the \gls{dwi} features 
$f(x)$ of the regions defined by the priors in \gls{dwi} space.
%
Using Gaussian distributions as region descriptors, we propose an
\gls{acwe}-like, piece-wise constant, variational image segmentation
model (where the unknown is the deformation field)
\cite{chan_active_2001}:

where $R$ indexes the existing regions and the integral domains
depend on the deformation field $u$. Note
that minimizing this energy, $\argmin_u\{E\}$, yields the \gls{map} 
estimate of a piece-wise smooth image model affected by Gaussian 
additive noise. This inverse problem is ill-posed
\cite{bertero_ill-posed_1988,hadamard_sur_1902}.
In order to account for deformation field regularity and to render the 
problem well-posed, we include limiting and regularization terms into 
the energy functional \cite{morozov_linear_1975,tichonov_solution_1963}:
%
\begin{align}
\label{eq:complete_energy}
E(u) &= \sum_{\forall{R}} \lbrace \int_{\Omega_R} (f-\mu_R)^T\Sigma_R^{-1}(f-\mu_R) dx \rbrace \nonumber \\
&\quad + \alpha \int  \|u\|^2 dx + \beta \int \left( \|\nabla u_x\|^2 + \|\nabla u_y\|^2 + \|\nabla u_z\|^2\right) dx
\end{align}
%
These regularity terms ensure that the segmenting contours in 
\gls{dwi} space are still close to their native shape. The model
easily allows to incorporate inhomogeneous and anisotropic 
regularization \cite{nagel_investigation_1986} to better regularize
the \gls{epi} distortion. \\
%

At each iteration, we update the distortion along the steepest 
energy descent. This gradient descent step can be efficiently 
tackled by discretizing the time in a forward Euler scheme, 
and making the right hand side semi-implicit in the 
regularization terms:
%
\begin{align}
\frac{u^{t+1}-u^t}{\tau} &= - \sum_{i=1}^{N_c} \left[ e(f(c_i'))  \hat{n}_{c_i'} \psi_{c_i}(x) \right] -\alpha u^{t+1} + \beta\Delta u^{t+1}
\end{align}
%
where the data terms remain functions of the current estimate 
$u^t$, thus $c_i' = c_i'(u^t)$. For simplicity on notation, we 
restricted the number of priors to only 1. We also defined 
$e(f(c_i')) = E_{out}(f(c_i')) - E_{in}(f(c_i'))$, 
and $E_R(f) = {(f-\mu_R)^T\Sigma_R^{-1}(f-\mu_R)}$.
We applied a spectral approach to solve this implicit scheme:
%
\begin{equation}
u^{t+1} = \mathcal{F}^{-1}\left\{ \frac{\mathcal{F}\{u^t/\tau
- \sum_{i=1}^{N_c} \left[e(f(c_i')) \hat{n}_{c_i'} \psi_{c_i}(x) \right]  \}}{\mathcal{F}\{(1/\tau+\alpha)\mathcal{I}-\beta\Delta\}} \right\}
\end{equation}
%

\subsection{Operator splitting}
In order to make the Energy minimisation computationally more tractable, we propose the following operator splitting: Let us optimize the data terms and the regularity terms on separate copies of the deformation field, now called $u$ and $v$, constrained to be equal:
\begin{align}
E(u,v) &= \int_{w'(u)} (f-\mu_w)^T\Sigma_w^{-1}(f-\mu_w) dx\nonumber\\
&\quad +\int_{g'(u)} (f-\mu_g)^T\Sigma_g^{-1}(f-\mu_g) dx\\
&\quad +\int_{o'(u)} (f-\mu_o)^T\Sigma_o^{-1}(f-\mu_o) dx\nonumber\\
&\quad +\int v^T A v dx + \int \tr\{(\nabla v^T)^T B (\nabla v^T)\} dx\nonumber
\end{align}
and now
\begin{equation}
\min_{u,v} \{ E \} \quad s.t. \quad u = v
\end{equation}
In order to take the equality constraint into account, we may make use of augmented Lagrangians (a combination of Lagrangian multipliers and penalty terms on the constraint)
\cite{bertsekas_multiplier_1976,glowinski_augmented_1989,nocedal_numerical_2006}:
\begin{equation}
AL(u,v,\lambda,r) = E(u,v) + \langle \lambda, u-v \rangle + \frac{r}{2} \| u - v \|_2^2
\end{equation}
To solve the constraint minimization problem, we may now optimize the Augmented Lagrangian in an iterative way:
\begin{equation}
\left\lbrace \begin{array}{rcl}
u^{t+1} &=& \argmin_{u} AL(u,v^t,\lambda^t,r)\\
v^{t+1} &=& \argmin_{v} AL(u^{t+1},v,\lambda^t,r)\\
\lambda^{t+1} &=& \lambda^t + \rho(u^{t+1}-v^{t+1})
\end{array}\right.\end{equation}
where typically $0 < \rho < r$. The two subminimization problems will now be much easier to handle than the original complete problem (``divide et impera'').

\subsection{Shape gradients}
To compute the gradient-descent of the data-term domain integrals with respect to the underlying deformation field, we want to make use of shape gradients \cite{Jehan-Besson2003,herbulot_segmentation_2006}. A little bit of theory is therefore in order.

Let $\Omega$ be an image domain and $\omega$ its boundary. Further, $r(x)$ is an ``arbitrary'' function over the image domain. We now derive the domain integral w.r.t. the contour evolution parameter $\tau$ (~ time):
\begin{equation}
\frac{\partial}{\partial \tau} \int_\Omega r(x) dx = \int_\Omega \frac{\partial r}{\partial \tau}(x) dx - \int_\omega r(x) \left\langle \frac{\partial\omega}{\partial\tau}, N_\omega\right\rangle dx
\end{equation}
where $\left\langle\frac{\partial\omega}{\partial\tau}, N_\omega\right\rangle$ is the projection of the boundary movement on the unit inward normal.


\subsection{Min w.r.t. $u$}
The first minimization problem optimizes the data-term. The problem is equivalent to minimizing the following energy:
\begin{align}
E(u) &= \int_{w'(u)} (f-\mu_w)^T\Sigma_w^{-1}(f-\mu_w) dx\nonumber\\
&\quad +\int_{g'(u)} (f-\mu_g)^T\Sigma_g^{-1}(f-\mu_g) dx\\
&\quad +\int_{o'(u)} (f-\mu_o)^T\Sigma_o^{-1}(f-\mu_o) dx\nonumber\\
&\quad + \langle \lambda, u-v \rangle + \frac{r}{2} \| u - v \|_2^2\nonumber
\end{align}
where we identify three instances of domain integrals of the form $\int_\Omega r(x) dx$. Optimality requires the derivative of this energy with respect to the parameters $u$ to be null. At this point, we may decide to ignore the influence of the boundary shift on the statistics of the regions (i.e. moving the boundary does not significantly impact the $\mu$ and $\Sigma$ descriptors). This means that we can drop the derivative of $r(x)$ w.r.t. contour evolution. What remains, are surface integrals at the two respective domain interfaces, $c'$ (wm/gm) and $d'$ (gm/CSF), plus the Lagrangian and penalty terms:
\begin{align}
\frac{\partial E}{\partial u_k^a} &= \int_{c'} \left[(f-\mu_g)^T\Sigma_g^{-1}(f-\mu_g) - (f-\mu_w)^T\Sigma_w^{-1}(f-\mu_w)\right]\left\langle\frac{\partial c'(s)}{\partial u_k^a}, N_{c'}(s)\right\rangle ds \nonumber\\
&\quad + \int_{d'} \left[(f-\mu_o)^T\Sigma_o^{-1}(f-\mu_o) - (f-\mu_g)^T\Sigma_g^{-1}(f-\mu_g)\right]\left\langle\frac{\partial d'(s)}{\partial u_k^a}, N_{d'}(s)\right\rangle ds \\
&\quad + \lambda_k^a + r(u_k^a - v_k^a)\nonumber\\
& = 0\nonumber
\end{align}
where $u_k^a$ is the $a$-th component of the parameter $u_k$, $a\in \{x,y,z\}$, $s$ is the surface parameter, $c'(s)$ and $d'(s)$ the corresponding points on the surfaces $c'$ and $d'$, and $N_{c'}(s)$ and $N_{d'}(s)$ the associated brainwise-inward unit normals.
Given the deformation field interpolation stated above, the boundary moves according to
\begin{equation}
\frac{\partial c'(s)}{\partial u_k^a} = \psi_k(c'(s))e_a
\end{equation}
where $e_a$ is the unit vector along direction $a$, and thus
\begin{equation}
\left\langle\frac{\partial c'(s)}{\partial u_k^a}, N_{c'}(s)\right\rangle = \psi_k(c'(s))N_{c'}^a(s)
\end{equation}
Now, we will discretize the surface integrals over $c'$ and $d'$ by simply summing over the nodes $c_i'$ and $d_i'$:
\begin{align}
\frac{\partial E}{\partial u_k^a} &= \sum_{i=1}^{N_c} \left[(f(c_i')-\mu_g)^T\Sigma_g^{-1}(f(c_i')-\mu_g) - (f(c_i')-\mu_w)^T\Sigma_w^{-1}(f(c_i')-\mu_w)\right]\psi_k(c_i')N_{c_i'}^a \nonumber\\
&\quad + \sum_{i=1}^{N_d} \left[(f(d_i')-\mu_o)^T\Sigma_o^{-1}(f(d_i')-\mu_o) - (f(d_i')-\mu_g)^T\Sigma_g^{-1}(f(d_i')-\mu_g)\right]\psi_k(d_i')N_{d_i'}^a \\
&\quad + \lambda_k^a + r(u_k^a - v_k^a)\nonumber\\
& = 0\nonumber
\end{align}
It is straightforward to solve this equation for $u_k^a$. The optimal distortion $u_k$ is found at each iteration as:
\begin{align}
u_k^{t+1} &= v_k^t - \frac{1}{r}\lambda_k^{t}\nonumber\\
&\quad - \frac{1}{r}\sum_{i=1}^{N_c} \left[(f(c_i')-\mu_g)^T\Sigma_g^{-1}(f(c_i')-\mu_g) - (f(c_i')-\mu_w)^T\Sigma_w^{-1}(f(c_i')-\mu_w)\right]\psi_k(c_i')N_{c_i'}\\
&\quad - \frac{1}{r}\sum_{i=1}^{N_d} \left[(f(d_i')-\mu_o)^T\Sigma_o^{-1}(f(d_i')-\mu_o) - (f(d_i')-\mu_g)^T\Sigma_g^{-1}(f(d_i')-\mu_g)\right]\psi_k(d_i')N_{d_i'}\nonumber
\end{align}

\subsection{Min w.r.t. $v$}

\textbf{It is important to realize that here we do not regularize the actual deformation field (i.e. after interpolation), but the underlying raw parameter field. }

For the optimization w.r.t. $v$, the relevant energy writes
\begin{align}
E(v) &= \int v^T A v dx + \int \tr\{(\nabla v^T)^T B (\nabla v^T)\} dx\\
&\quad + \langle \lambda, u-v \rangle + \frac{r}{2} \| u - v \|_2^2\nonumber
\end{align}
Let's assume the simplest, homogeneous isotropic case, $A = \alpha/2$ and $B = \beta/2$. The associated Euler-Lagrange equation is found as:
\begin{equation}
\alpha v - \beta\Delta v + rv = ru + \lambda
\end{equation}
which easily translates into Fourier domain:
\begin{equation}
v^{t+1} = \mathcal{F}^{-1}\left\{ \frac{\mathcal{F}\{ru + \lambda\}}{\mathcal{F}\{(\alpha+r)\mathcal{I}-\beta\Delta\}} \right\}
\end{equation}
where $\mathcal{I}$ denotes the identity operator.

Here, we rewrite the Laplacian as a linear combination of the identity and shift operators:
\begin{equation}
\Delta = \mathcal{S}_x^- + \mathcal{S}_x^+ + \mathcal{S}_y^- + \mathcal{S}_y^+ - 4 \mathcal{I}
\end{equation}
where $\mathcal{S}_{x,y}^{\pm}$ stands for the forward ($+$) and backward ($-$) shift operator along $x$ or $y$, respectively, of which the Fourier transform is found easily as
\begin{equation}
\mathcal{F}\{\mathcal{S}_{x,y}^{\pm}\} = e^{\pm i\omega_{x,y}},
\end{equation}
where $\omega_{x,y}$ is the normalized pulsation along $x$- and $y$-direction. Accordingly, the Fourier transform of the discrete Laplacian is found as
\begin{align}
\mathcal{F}\{\Delta\} &= e^{-i\omega_x } + e^{i\omega_x } + e^{-i\omega_y } + e^{i\omega_y } - 4\nonumber\\
&= 2\left( \cos(\omega_x) + \cos(\omega_y) - 2 \right)
\end{align}

The remaining transforms are trivial or can be computed using FFT (as in \citep{estellers_efficient_2011}).


\subsection{Lagrangian multiplier update}
At each iteration, the Lagrangian multipliers are updated as noted before:
\begin{equation}
\lambda^{t+1} = \lambda^t + \rho(u^{t+1}-v^{t+1})
\end{equation}

\subsection{Region descriptor reestimation}
In regular intervals, i.e. after $n$ iterations, the parameters $\mu$ and $\Sigma$ of the involved regions need to be reestimated based on the shifted volumetric samples $w_j'$, $g_j'$ and $o_j'$.

\subsection{Convergence}
In order to ``fixate'' the evolution when close to convergence, it is advised to slightly increase the penalty weight $r$ at each iteration. Note that as can be seen in the above equations, $r$ governs the step-size or  leash-length at each iteration, i.e. the amount by which the new estimate $u$ may move away from the preceding $v$ and vice-versa. 


\subsection{Alternative route: Gradient descent}
If one absolutely wants to avoid the operator splitting and the augmented Lagrangians, then simple gradient descent may work as well (at least under the same simple model circumstances).

At each iteration, we update the distortion along the steepest energy descent:
\begin{equation}
\frac{\partial u_k^t}{\partial t} = -\frac{\partial E(u)}{\partial u_k^t}
\end{equation}
At this point, we interpret the parameter field $u_k$ to be a continuous function $u(x)$, sampled at the locations $x_k$, and determine the gradient-descent equation\footnote{The same assumption was being made above in the minimization of the regularity terms}:
\begin{align}
\frac{\partial u^t}{\partial t} &= - \sum_{i=1}^{N_c} \left[(f(c_i')-\mu_g)^T\Sigma_g^{-1}(f(c_i')-\mu_g) - (f(c_i')-\mu_w)^T\Sigma_w^{-1}(f(c_i')-\mu_w)\right]\psi_{c_i}(x)N_{c_i'}\nonumber\\
&\quad -\sum_{i=1}^{N_d} \left[(f(d_i')-\mu_o)^T\Sigma_o^{-1}(f(d_i')-\mu_o) - (f(d_i')-\mu_g)^T\Sigma_g^{-1}(f(d_i')-\mu_g)\right]\psi_{d_i}(x)N_{d_i'}\\
&\quad -\alpha u + \beta\Delta u\nonumber
\end{align}
where we have swapped $\psi_k(c_i')$ into $\psi_{c_i}(x)$.

This gradient descent step can be efficiently tackled by discretizing the time in a forward Euler scheme, and making the right hand side semi-implicit in the regularization terms:
\begin{align}
\frac{u^{t+1}-u^t}{\tau} &= - \sum_{i=1}^{N_c} \left[(f(c_i')-\mu_g)^T\Sigma_g^{-1}(f(c_i')-\mu_g) - (f(c_i')-\mu_w)^T\Sigma_w^{-1}(f(c_i')-\mu_w)\right]\psi_{c_i}(x)N_{c_i'}\nonumber\\
&\quad -\sum_{i=1}^{N_d} \left[(f(d_i')-\mu_o)^T\Sigma_o^{-1}(f(d_i')-\mu_o) - (f(d_i')-\mu_g)^T\Sigma_g^{-1}(f(d_i')-\mu_g)\right]\psi_{d_i}(x)N_{d_i'}\nonumber\\
&\quad -\alpha u^{t+1} + \beta\Delta u^{t+1}
\end{align}
where the data terms remain functions of the current estimate $u^t$, i.e. all $c_i' = c_i'(u^t)$ and $d_i' = d_i'(u^t)$. Again, we propose a spectral approach to solve this implicit scheme:
\begin{equation}
u^{t+1} = \mathcal{F}^{-1}\left\{ \frac{\mathcal{F}\{u^t/\delta - \sum_{i=1}^{N_c}(\ldots) - \sum_{i=1}^{N_d}(\ldots)  \}}{\mathcal{F}\{(1/\delta+\alpha)\mathcal{I}-\beta\Delta\}} \right\}
\end{equation}
It is easily verified, that the same update can be obtained by plugging the AL-update w.r.t. $u$ into the AL-update w.r.t. $v$, and by identifying $r = 1/\delta$ (the only exception is the distortion on which the data-term is being evaluated).
